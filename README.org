-*- Mode: org; mode: auto-fill; fill-column: 76; org-download-image-dir: "~/Documents/leandro/scrapping-tutorial/img" -*-

* Scraping tutorial
  

     This tutorial is based on [[Scrapy Tutorial: Web Scraping
      Craigslist][https://python.gotrained.com/scrapy-tutorial-web-scraping-craigslist/]]
  
     Learn objectives:
     * Get familiar with Scrapy. Scrapy is one of the most popular and powerful
       Python scraping frameworks.
     * I did my first web scrapping about 2008 using beautiful soup and I want
       to see how the technology changed
     * Get familiar with scrapping craiglist and prepare for the Housing Market
       scrapping project

       

** Tutorial

   
*** create the project and spiders

      #+begin_src sh
      scrapy startproject craigslist
      cd craiglist
      scrapy genspider jobs https://newyork.craigslist.org/search/egr
      #+end_src


    See the project folders
      #+begin_src sh :results output
       tree
      #+end_src

      #+RESULTS:
      #+begin_example
      .
      ├── craigslist
      │   ├── craigslist
      │   │   ├── __init__.py
      │   │   ├── items.py
      │   │   ├── middlewares.py
      │   │   ├── pipelines.py
      │   │   ├── __pycache__
      │   │   │   ├── __init__.cpython-36.pyc
      │   │   │   └── settings.cpython-36.pyc
      │   │   ├── settings.py
      │   │   └── spiders
      │   │       ├── __init__.py
      │   │       ├── jobs.py
      │   │       └── __pycache__
      │   │           └── __init__.cpython-36.pyc
      │   └── scrapy.cfg
      ├── LICENSE
      └── README.org

      5 directories, 13 files
      #+end_example
      
      The commands creates all the folders and the spider code

      #+begin_src python
        # -*- coding: utf-8 -*-
        import scrapy

        class JobsSpider(scrapy.Spider):
            name = 'jobs'  # spider name

            # the list of the domains that the spider is allowed scrape.
            allowed_domains = ['craigslist.org']

            # the list of one or more URL(s) with which the spider starts crawling.
            start_urls = ['https://newyork.craigslist.org/search/egr/']

            # main pider function used to extract the target data of the web page
            def parse(self, response):
                pass

      #+end_src

*** Scrapping all post of one page
    
    #+DOWNLOADED: /tmp/screenshot.png @ 2019-01-16 11:00:56
    [[file:Scraping%20tutorial/screenshot_2019-01-16_11-00-56.png]]
    
    Based on the image we need to get extract all elements <li class="result-row" data-pid="6796431994"> or we can even start from <p class="result-info">:
    #+BEGIN_SRC html
      <ul class="row">
        <li class="result-row" data-pid="6796431994">
              <a href="https://newyork.craigslist.org/mnh/egr/d/brooklyn-mechanical-engineer/6796431994.html" class="result-image gallery empty"></a>

          <p class="result-info">
              <span class="icon icon-star" role="button" title="save this post in your favorites list">
                  <span class="screen-reader-text">favorite this post</span>
              </span>

                  <time class="result-date" datetime="2019-01-16 12:52" title="Wed 16 Jan 12:52:59 PM">Jan 16</time>

              <a href="https://newyork.craigslist.org/mnh/egr/d/brooklyn-mechanical-engineer/6796431994.html" data-id="6796431994" class="result-title hdrlnk">Mechanical Engineer Consultant...School Inspections...NYC</a>


              <span class="result-meta">

                      <span class="result-hood"> (New York)</span>

                      <span class="result-tags">
                          <span class="maptag" data-pid="6796431994">map</span>
                      </span>

                      <span class="banish icon icon-trash" role="button">
                          <span class="screen-reader-text">hide this posting</span>
                      </span>

                  <span class="unbanish icon icon-trash red" role="button" aria-hidden="true"></span>
                  <a href="#" class="restore-link">
                      <span class="restore-narrow-text">restore</span>
                      <span class="restore-wide-text">restore this posting</span>
                  </a>

              </span>
          </p>
      </li>
        <!-- all other posts -->
      </ul>
    #+END_SRC
    
    To extract we can use the Xpath expression (see https://devhints.io/xpath#prefixes). Xpath is language to xtract/access xml path in a document.


    #+begin_src python
      def parse(self, response):

          # mean extract all html tag <p> with attribute class="result-info". // mean
          # the patrh start from the html root tag <html>
          jobs = response.xpath('//p[@class="result-info"]')

          # extract titles, addres n urls

          for job in jobs:
              title = job.xpath('a/text()').extract_first()
              address = job.xpath('span[@class="result-meta"]/span[@class="result-hood"]/text()').extract_first("")[2:-1]
              relative_url = job.xpath('a/@href').extract_first()
              absolute_url = response.urljoin(relative_url)

              yield{'URL':absolute_url, 'Title':title, 'Address':address}

    #+end_src


    Run form scrapping-tutorial/craiglist/
    
    #+begin_src sh
    scrapy crawl jobs -o result-jobs-one-page.csv
    #+end_src

    Some outputs notes
    #+BEGIN_EXAMPLE
      2019-01-16 11:32:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://newyork.craigslist.org/search/egr/>  <= That is ok
      {'URL': 'https://newyork.craigslist.org/mnh/egr/d/facade-building-envelope-structural/6787112674.html', 'Title': 'Facade / Building Envelope Structural Project Engineer', 'Address': 'TriBeCa'}
      2019-01-16 11:32:38 [scrapy.core.engine] INFO: Closing spider (finished)
      2019-01-16 11:32:38 [scrapy.extensions.feedexport] INFO: Stored csv feed (120 items) in: result-jobs-one-page.csv
      2019-01-16 11:32:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:  
        {'downloader/request_bytes': 505,
       'downloader/request_count': 2,
       'downloader/request_method_count/GET': 2,
       'downloader/response_bytes': 21745,
       'downloader/response_count': 2,
       'downloader/response_status_count/200': 2,
       'finish_reason': 'finished',  <= good finished whithout erros
       'finish_time': datetime.datetime(2019, 1, 16, 19, 32, 38, 312002),
       'item_scraped_count': 120, <= refers to the number of titles scraped from the page
       'log_count/DEBUG': 123,
       'log_count/INFO': 8,
       'memusage/max': 54525952,
       'memusage/startup': 54525952,
       'response_received_count': 2,
       'scheduler/dequeued': 1,
       'scheduler/dequeued/memory': 1,
       'scheduler/enqueued': 1,
       'scheduler/enqueued/memory': 1,
       'start_time': datetime.datetime(2019, 1, 16, 19, 32, 37, 526258)}
      2019-01-16 11:32:38 [scrapy.core.engine] INFO: Spider closed (finished)

    #+END_EXAMPLE
    
** References

   This tutorial is based on:
   * Tutorial:  [[Scrapy Tutorial: Web Scraping Craigslist][https://python.gotrained.com/scrapy-tutorial-web-scraping-craigslist/]]
   * Code: https://github.com/GoTrained/Scrapy-Craigslist/
   * Scrapped web site: https://newyork.craigslist.org/search/egr
   * XPath cheat sheet: https://devhints.io/xpath#prefixes
