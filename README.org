-*- Mode: org; mode: auto-fill; fill-column: 76; org-download-image-dir: "~/Documents/leandro/scrapping-tutorial/img" -*-

* Scraping tutorial: Web Scraping Craigslist

** Introduction

   This tutorial is based on [[https://python.gotrained.com/scrapy-tutorial-web-scraping-craigslist/]]
  
   Learn objectives:
      * Get familiar with Scrapy. Scrapy is one of the most popular and powerful
        Python scraping frameworks.
      * I did my first web scrapping about 2008 using beautiful soup and I want
        to see how the technology changed
      * Get familiar with scrapping craiglist and prepare for the Housing Market
        scrapping project
       
** create the project and spiders

     #+begin_src sh
     scrapy startproject craigslist
     cd craiglist
     scrapy genspider jobs https://newyork.craigslist.org/search/egr
     #+end_src


   See the project folders
     #+begin_src sh :results output
      tree
     #+end_src

     #+RESULTS:
     #+begin_example
     .
     ├── craigslist
     │   ├── craigslist
     │   │   ├── __init__.py
     │   │   ├── items.py
     │   │   ├── middlewares.py
     │   │   ├── pipelines.py
     │   │   ├── __pycache__
     │   │   │   ├── __init__.cpython-36.pyc
     │   │   │   └── settings.cpython-36.pyc
     │   │   ├── settings.py
     │   │   └── spiders
     │   │       ├── __init__.py
     │   │       ├── jobs.py
     │   │       └── __pycache__
     │   │           └── __init__.cpython-36.pyc
     │   └── scrapy.cfg
     ├── LICENSE
     └── README.org

     5 directories, 13 files
     #+end_example
      
     The commands creates all the folders and the spider code

     #+begin_src python
       # -*- coding: utf-8 -*-
       import scrapy

       class JobsSpider(scrapy.Spider):
           name = 'jobs'  # spider name

           # the list of the domains that the spider is allowed scrape.
           allowed_domains = ['craigslist.org']

           # the list of one or more URL(s) with which the spider starts crawling.
           start_urls = ['https://newyork.craigslist.org/search/egr/']

           # main pider function used to extract the target data of the web page
           def parse(self, response):
               pass

     #+end_src

** Scrapping all post of one page
    
   #+DOWNLOADED: /tmp/screenshot.png @ 2019-01-16 11:00:56
   [[file:Scraping%20tutorial/screenshot_2019-01-16_11-00-56.png]]
    
   Based on the image we need to get extract all elements <li class="result-row" data-pid="6796431994"> or we can even start from <p class="result-info">:
   #+BEGIN_SRC html
     <ul class="row">
       <li class="result-row" data-pid="6796431994">
             <a href="https://newyork.craigslist.org/mnh/egr/d/brooklyn-mechanical-engineer/6796431994.html" class="result-image gallery empty"></a>

         <p class="result-info">
             <span class="icon icon-star" role="button" title="save this post in your favorites list">
                 <span class="screen-reader-text">favorite this post</span>
             </span>

                 <time class="result-date" datetime="2019-01-16 12:52" title="Wed 16 Jan 12:52:59 PM">Jan 16</time>

             <a href="https://newyork.craigslist.org/mnh/egr/d/brooklyn-mechanical-engineer/6796431994.html" data-id="6796431994" class="result-title hdrlnk">Mechanical Engineer Consultant...School Inspections...NYC</a>


             <span class="result-meta">

                     <span class="result-hood"> (New York)</span>

                     <span class="result-tags">
                         <span class="maptag" data-pid="6796431994">map</span>
                     </span>

                     <span class="banish icon icon-trash" role="button">
                         <span class="screen-reader-text">hide this posting</span>
                     </span>

                 <span class="unbanish icon icon-trash red" role="button" aria-hidden="true"></span>
                 <a href="#" class="restore-link">
                     <span class="restore-narrow-text">restore</span>
                     <span class="restore-wide-text">restore this posting</span>
                 </a>

             </span>
         </p>
     </li>
       <!-- all other posts -->
     </ul>
   #+END_SRC
    
   To extract we can use the Xpath expression (see https://devhints.io/xpath#prefixes). Xpath is language to xtract/access xml path in a document.


   #+begin_src python
     def parse(self, response):

         # mean extract all html tag <p> with attribute class="result-info". // mean
         # the patrh start from the html root tag <html>
         jobs = response.xpath('//p[@class="result-info"]')

         # extract titles, addres n urls

         for job in jobs:
             title = job.xpath('a/text()').extract_first()
             address = job.xpath('span[@class="result-meta"]/span[@class="result-hood"]/text()').extract_first("")[2:-1]
             relative_url = job.xpath('a/@href').extract_first()
             absolute_url = response.urljoin(relative_url)

             yield{'URL':absolute_url, 'Title':title, 'Address':address}

   #+end_src

   Run from scrapping-tutorial/craiglist/
    
   #+begin_src sh
   scrapy crawl jobs -o result-jobs-one-page.csv
   #+end_src

   Some outputs notes
   #+BEGIN_EXAMPLE
     2019-01-16 11:32:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://newyork.craigslist.org/search/egr/>  <= That is ok
     {'URL': 'https://newyork.craigslist.org/mnh/egr/d/facade-building-envelope-structural/6787112674.html', 'Title': 'Facade / Building Envelope Structural Project Engineer', 'Address': 'TriBeCa'}
     2019-01-16 11:32:38 [scrapy.core.engine] INFO: Closing spider (finished)
     2019-01-16 11:32:38 [scrapy.extensions.feedexport] INFO: Stored csv feed (120 items) in: result-jobs-one-page.csv
     2019-01-16 11:32:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:  
       {'downloader/request_bytes': 505,
      'downloader/request_count': 2,
      'downloader/request_method_count/GET': 2,
      'downloader/response_bytes': 21745,
      'downloader/response_count': 2,
      'downloader/response_status_count/200': 2,
      'finish_reason': 'finished',  <= good finished whithout erros
      'finish_time': datetime.datetime(2019, 1, 16, 19, 32, 38, 312002),
      'item_scraped_count': 120, <= refers to the number of titles scraped from the page
      'log_count/DEBUG': 123,
      'log_count/INFO': 8,
      'memusage/max': 54525952,
      'memusage/startup': 54525952,
      'response_received_count': 2,
      'scheduler/dequeued': 1,
      'scheduler/dequeued/memory': 1,
      'scheduler/enqueued': 1,
      'scheduler/enqueued/memory': 1,
      'start_time': datetime.datetime(2019, 1, 16, 19, 32, 37, 526258)}
     2019-01-16 11:32:38 [scrapy.core.engine] INFO: Spider closed (finished)

   #+END_EXAMPLE

** Scrapping multiple pages    
   
   To do the same for all the result pages of Craigslist’s Architecture &
   Engineering jobs, you need to extract the “next” URLs and then apply the
   same parse() function on them.


   Click right button in the next buttom and select inspect
   #+BEGIN_SRC html
     <a href="/search/egr?s=120" class="button next" title="next page">next &gt; </a>
   #+END_SRC
    
   Add this code

   #+begin_src python
     # -*- coding: utf-8 -*-
     import scrapy
     from scrapy import Request

     class JobsSpider(scrapy.Spider):
         name = "jobsall"
         allowed_domains = ["craigslist.org"]
         start_urls = ["https://newyork.craigslist.org/search/egr"]

         def parse(self, response):
             jobs = response.xpath('//p[@class="result-info"]')

             # extract titles, addres n urls
             for job in jobs:

                 title = job.xpath('a/text()').extract_first()

                 # extract_first("") which means if there is no result, the result is “”.
                 # [2:-1] removes the parenthesis
                 address = job.xpath('span[@class="result-meta"]/span[@class="result-hood"]/text()').extract_first("")[2:-1]

                 relative_url = job.xpath('a/@href').extract_first()

                 absolute_url = response.urljoin(relative_url)

                 yield{'URL':absolute_url, 'Title':title, 'Address':address}

             # scrap next page
             relative_next_url = response.xpath('//a[@class="button next"]/@href').extract_first()
             absolute_next_url = response.urljoin(relative_next_url)
             yield Request(absolute_next_url, callback=self.parse)
   #+end_src

   Run form scrapping-tutorial/craiglist/
    
   #+begin_src sh
   scrapy crawl jobsall -o result-jobs-multi-pages.csv
   #+end_src

   #+BEGIN_EXAMPLE
     2019-01-16 22:32:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
     {'downloader/request_bytes': 835,
      'downloader/request_count': 3,
      'downloader/request_method_count/GET': 3,
      'downloader/response_bytes': 41298,
      'downloader/response_count': 3,
      'downloader/response_status_count/200': 3,
      'dupefilter/filtered': 1,
      'finish_reason': 'finished',
      'finish_time': datetime.datetime(2019, 1, 17, 6, 32, 17, 16011),
      'item_scraped_count': 223,  <==========================
      'log_count/DEBUG': 228,
      'log_count/INFO': 8,
      'memusage/max': 54484992,
      'memusage/startup': 54484992,
      'request_depth_max': 2,
      'response_received_count': 3,
      'scheduler/dequeued': 2,
      'scheduler/dequeued/memory': 2,
      'scheduler/enqueued': 2,
      'scheduler/enqueued/memory': 2,
      'start_time': datetime.datetime(2019, 1, 17, 6, 32, 15, 471444)}
     2019-01-16 22:32:17 [scrapy.core.engine] INFO: Spider closed (finished)

   #+END_EXAMPLE

   223 job posting was scrapped

** Scrapping all post and job description 

   In this spider, we will open the URL of each job and scrape its description.
   So we are going to create the function parse_page that knows how to parse
   job description page
    
   To check page with description. Open the first entry: https://newyork.craigslist.org/mnh/egr/d/brooklyn-engineer-manager-arlo-nomad/6796659355.html


   #+DOWNLOADED: /tmp/screenshot.png @ 2019-01-16 22:44:27
   [[file:Scraping%20tutorial/screenshot_2019-01-16_22-44-27.png]]
    
   the html tag of interested is:

   #+BEGIN_SRC html
     <section id="posting body">

       <!-- few html tags (most div) -->

       Arlo Hotels is an independent lifestyle hotel now actively seeking a dynamic
       Engineer Manager for our Arlo NoMad property! <br>
       <br>
       Are you someone who is passionate about People, driven by Purpose and
       Clever in your approach? If so keep on reading!! Here at Arlo we strive to
       create a sense of awe that leaves those we touch wanting more"...... <br>
       <br>
       SUMMARY DESCRIPTION: <br>
       <br>
       The Engineer Manager is responsible for effectively leading the
       engineering team in the execution of all maintenance of the overall hotel.
       From essential upgrading, installation and necessary purchase of all HVAC
       systems, mechanical, electrical and related equipment. <br>

       <!-- More text and the END -->
  
     </section>
   #+END_SRC

   For compensation and employment type:
   #+BEGIN_SRC html
     <span>compensation: <b>$65,000.00 - $75,000.00 USD</b></span>
     <span>employment type: <b>full-time</b></span>
   #+END_SRC
    
   The code:
   #+begin_src python
     import scrapy
     from scrapy import Request

     class JobsSpider(scrapy.Spider):
         name = "jobscontent"
         allowed_domains = ["craigslist.org"]
         start_urls = ["https://newyork.craigslist.org/search/egr"]

         def parse(self, response):
             jobs = response.xpath('//p[@class="result-info"]')

             # extract titles, addres n urls
             for job in jobs:

                 title = job.xpath('a/text()').extract_first()

                 # extract_first("") which means if there is no result, the result is “”.
                 # [2:-1] removes the parenthesis
                 address = job.xpath('span[@class="result-meta"]/span[@class="result-hood"]/text()').extract_first("")[2:-1]

                 relative_url = job.xpath('a/@href').extract_first()

                 absolute_url = response.urljoin(relative_url)

                 yield{'URL':absolute_url, 'Title':title, 'Address':address}

             # scrap next page
             relative_next_url = response.xpath('//a[@class="button next"]/@href').extract_first()
             absolute_next_url = response.urljoin(relative_next_url)

             # call parse_page funtion and pass the metada to the function
             yield Request(absolute_url, callback=self.parse_page, meta={'URL': absolute_url, 'Title': title, 'Address':address})


         def parse_page(self, response):
             # parse page with description

             # acces the metada passed
             url = response.meta.get('URL')
             title = response.meta.get('Title')
             address = response.meta.get('Address')

             description = "".join(line for line in response.xpath('//*[@id="postingbody"]/text()').extract()).strip()
             compensation = response.xpath('//p[@class="attrgroup"]/span[1]/b/text()').extract_first()
             employment_type  = response.xpath('//p[@class="attrgroup"]/span[2]/b/text()').extract_first()

             yield{'URL': url, 'Title': title, 'Address':address, 'Description':description, 'Compensation':compensation, 'Employment Type':employment_type}
   #+end_src

   Run from scrapping-tutorial/craiglist/. Testing json output
   #+begin_src sh
   scrapy crawl jobscontent -o result-jobs-multi-pages-content.json
   #+end_src
    
   #+BEGIN_EXAMPLE
     2019-01-16 22:52:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
     {'downloader/request_bytes': 883,
      'downloader/request_count': 3,
      'downloader/request_method_count/GET': 3,
      'downloader/response_bytes': 26750,
      'downloader/response_count': 3,
      'downloader/response_status_count/200': 3,
      'finish_reason': 'finished',
      'finish_time': datetime.datetime(2019, 1, 17, 6, 52, 58, 215687),
      'item_scraped_count': 121,   <====
      'log_count/DEBUG': 125,
      'log_count/INFO': 8,
      'memusage/max': 54603776,
      'memusage/startup': 54603776,
      'request_depth_max': 1,
      'response_received_count': 3,
      'scheduler/dequeued': 2,
      'scheduler/dequeued/memory': 2,
      'scheduler/enqueued': 2,
      'scheduler/enqueued/memory': 2,
      'start_time': datetime.datetime(2019, 1, 17, 6, 52, 56, 908061)}
     2019-01-16 22:52:58 [scrapy.core.engine] INFO: Spider closed (finished)
   #+END_EXAMPLE

   #+BEGIN_SRC json
     [
     {"URL": "https://newyork.craigslist.org/mnh/egr/d/brooklyn-engineer-manager-arlo-nomad/6796659355.html", "Title": "Engineer Manager - Arlo NoMad", "Address": "Flatiron"},
     {"URL": "https://newyork.craigslist.org/que/egr/d/junior-architect/6796608638.html", "Title": "JUNIOR ARCHITECT", "Address": "NEW YORK"},
     {"URL": "https://newyork.craigslist.org/que/egr/d/astoria-draft-person-certified-arch-cad/6796590945.html", "Title": "Draft Person Certified Arch Cad  Const. Safety Plans", "Address": "Queens"},
     {"URL": "https://newyork.craigslist.org/fct/egr/d/engineer-designer-electrical/6796503693.html", "Title": "Engineer/Designer Electrical", "Address": "Norwalk, CT"},
     {"URL": "https://newyork.craigslist.org/mnh/egr/d/brooklyn-mechanical-engineer/6796431994.html", "Title": "Mechanical Engineer Consultant...School Inspections...NYC", "Address": "New York"},
     {"URL": "https://newyork.craigslist.org/mnh/egr/d/harriman-professional-land-surveyor/6796408356.html", "Title": "Professional Land Surveyor", "Address": "Harriman"},
     {"URL": "https://newyork.craigslist.org/mnh/egr/d/new-york-city-experienced-construction/6796336132.html", "Title": "Experienced CONSTRUCTION Project Manager", "Address": "Midtown West"},
     {"URL": "https://newyork.craigslist.org/mnh/egr/d/new-york-city-senior-project-manager/6796329945.html", "Title": "Senior Project Manager", "Address": ""},
     {"URL": "https://newyork.craigslist.org/mnh/egr/d/intermediate-architect/6796326890.html", "Title": "Intermediate Architect", "Address": "Flatiron"}
     ]
   #+END_SRC

   #+RESULTS:
    
** References

   This tutorial is based on:
   * Scrapy: https://scrapy.org/
   * Tutorial: Web Scraping Craigslist: https://python.gotrained.com/scrapy-tutorial-web-scraping-craigslist/
   * Code: https://github.com/GoTrained/Scrapy-Craigslist/
   * Scrapped web site: https://newyork.craigslist.org/search/egr
   * XPath cheat sheet: https://devhints.io/xpath#prefixes


   
   
